{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be34293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "#match all words with a-z or A-Z with length 3+ and then ends with another \n",
    "# a-z character. But returns only the first part (the capture group) -\n",
    "# it will chop off the last character\n",
    "dickens_text_df[\"line\"].str.extractall(r'([A-Za-z]{3,})[A-Z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45588324",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A thorough examination of the movie shows Thor was a thorn in the side of the villains, both then and now. thor.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6e84d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thor', 'thor']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\b(?:t|T)hor\\b', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69182383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 't']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\b(t|T)hor\\b', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296eb31f",
   "metadata": {},
   "source": [
    "### NLTK's `PunktSentenceTokenizer`\n",
    "\n",
    "> `PunktSentenceTokenizer` is an **sentence boundary detection algorithm** that must be trained to be used. NLTK already includes a pre-trained version of the `PunktSentenceTokenizer` ([StackOverflow](https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk))\n",
    "\n",
    "for word --> word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An F1 score is often a good measure\n",
    "# our dataset target is class imbalanced (ie. 96% positive, 4% negative)\n",
    "# when we want to balance optimizing for both precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df91f3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>1890</th>\n",
       "      <th>1912</th>\n",
       "      <th>1940</th>\n",
       "      <th>1984</th>\n",
       "      <th>31</th>\n",
       "      <th>420</th>\n",
       "      <th>43</th>\n",
       "      <th>aegean</th>\n",
       "      <th>african</th>\n",
       "      <th>...</th>\n",
       "      <th>william</th>\n",
       "      <th>wishbone</th>\n",
       "      <th>with</th>\n",
       "      <th>woman</th>\n",
       "      <th>women</th>\n",
       "      <th>world</th>\n",
       "      <th>young</th>\n",
       "      <th>yuai</th>\n",
       "      <th>yum</th>\n",
       "      <th>zappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 619 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     10  1890  1912  1940  1984  31  420  43  aegean  african  ...  william  \\\n",
       "0     0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "1     0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "2     0     0     0     0     0   0    0   0       0        0  ...        1   \n",
       "3     0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "4     0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "..   ..   ...   ...   ...   ...  ..  ...  ..     ...      ...  ...      ...   \n",
       "685   0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "686   0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "687   0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "688   0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "689   0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "\n",
       "     wishbone  with  woman  women  world  young  yuai  yum  zappa  \n",
       "0           0     0      0      0      0      0     0    0      0  \n",
       "1           0     0      0      0      0      0     0    0      0  \n",
       "2           0     0      0      0      0      0     0    0      0  \n",
       "3           0     0      0      0      0      0     0    0      0  \n",
       "4           0     0      0      0      0      0     0    0      0  \n",
       "..        ...   ...    ...    ...    ...    ...   ...  ...    ...  \n",
       "685         0     0      0      0      0      0     0    0      0  \n",
       "686         0     0      0      0      0      0     0    0      0  \n",
       "687         0     0      0      0      0      0     0    0      0  \n",
       "688         0     0      0      0      0      0     0    0      0  \n",
       "689         0     0      0      0      0      0     0    0      0  \n",
       "\n",
       "[690 rows x 619 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import in a dataset of baltimore's public art galleries\n",
    "public_art_df: pd.DataFrame = pd.read_csv(\"./datasets/baltimore_public_art.csv\")\n",
    "\n",
    "public_art_df = public_art_df.replace(np.nan, '', regex=True)\n",
    "public_art_df.head()\n",
    "# use the titleOfArtwork field\n",
    "titles_of_artworks: pd.Series = public_art_df[\"titleOfArtwork\"]\n",
    "titles_of_artworks\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(titles_of_artworks)\n",
    "corpus_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb95cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes --> labelling needed.\n",
    "# Label product page or not?\n",
    "# Given certain words --> is it product page or not?\n",
    "# Caveat - picture description\n",
    "\n",
    "# When creating Naive Bayes, need count vectorization\n",
    "# # # use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=0.01) \n",
    "\n",
    "# # # use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# # # and keep only the top 200\n",
    "# vectorizer = CountVectorizer(stop_words=\"english\", binary=True, ma=2, max_features=200) \n",
    "\n",
    "# # # use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# # # and keep only the top 200\n",
    "\n",
    "# Cosine similarity\n",
    "# Is the product description similar to each other?\n",
    "\n",
    "# TF-IDF - product page "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec94b2",
   "metadata": {},
   "source": [
    "# Removing Stopwords Using `gensim`\n",
    "\n",
    "Removing stopwords in `nltk` often means you first have to tokenize the document into distinct tokens, then run each token through to check if it is a stopword. Another commonly used NLP library in Python, `gensim`, has a helper function to do this all in one go:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5765608e",
   "metadata": {},
   "source": [
    "# Finding Similar Word Matches Using `difflib`\n",
    "Within Python's Standard Library, the `difflib` has a variety of tools for helping identify differences between text and content. It uses an algorithm called the **Ratcliff-Obershelp algorithm**, which is described in brief below:\n",
    "\n",
    "> The idea is to find the longest contiguous matching subsequence that contains no “junk” elements; these “junk” elements are ones that are uninteresting in some sense, such as blank lines or whitespace. (Handling junk is an extension to the Ratcliff and Obershelp algorithm.) The same idea is then applied recursively to the pieces of the sequences to the left and to the right of the matching subsequence. This does not yield minimal edit sequences, but does tend to yield matches that “look right” to people. [Link](https://docs.python.org/3/library/difflib.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd68ac88",
   "metadata": {},
   "source": [
    "## Fuzzy Matching\n",
    "Fuzzy matching refers to \"approximate matching\", where we are allowed a certain degree of error between the query value and the search result. \n",
    "\n",
    "The `fuzzywuzzy` library uses a distance measure called **Levenshtein Distance** which describes the minimum number of operations to transform one string into another.\n",
    "\n",
    "* `cat` $\\rightarrow$ `cat` : `0` distance\n",
    "* `dog` $\\rightarrow$ `door`: `2` distance\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "* spell checking\n",
    "* DNA analysis\n",
    "* authorship/plagiarism detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e26ebe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f66dbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"dog\", \"hog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90e6113a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"dog\", \"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426bd69",
   "metadata": {},
   "source": [
    "## Simple Optimizations to Improve Naive Bayes Probabilistic Models for Text Classification\n",
    "\n",
    "- to may be useful to simply create a simple **co-occurence matrix**, and **run a correlation analysis** on the features (words). If certain words have extremely high correlations, you may wish to take them out, or fuse them into a single entity.\n",
    "- apply smoothing techniques to handle **out-of-vocabulary test words**\n",
    "- **ensemble techniques like bagging / boosting** do **not** help. There isn't any \"variation\" in a Naive Bayes model. Given the same trained corpus $C$, and a new text message $m$, a Naive Bayes model will always output the same prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de99886",
   "metadata": {},
   "source": [
    "## Generating Bigrams Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd7aec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'wanted'), ('wanted', 'to'), ('to', 'grab'), ('grab', 'breakfast'), ('breakfast', 'one'), ('one', 'morning'), ('morning', 'before'), ('before', 'work'), ('work', 'since'), ('since', 'it')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "reviews_df = pd.read_csv(\"./datasets/mcdonalds-yelp-negative-reviews.csv\", encoding=\"latin-1\")\n",
    "for review in reviews_df[\"review\"]:    \n",
    "    bigram = list(nltk.bigrams(word_tokenize(review)))\n",
    "print(bigram[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b0328",
   "metadata": {},
   "source": [
    "## Generating Bigrams Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0cc533a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00 am</th>\n",
       "      <th>00 for</th>\n",
       "      <th>00 in</th>\n",
       "      <th>00 meal</th>\n",
       "      <th>00 pm</th>\n",
       "      <th>00 sunday</th>\n",
       "      <th>000 mile</th>\n",
       "      <th>00am and</th>\n",
       "      <th>00am on</th>\n",
       "      <th>00am service</th>\n",
       "      <th>...</th>\n",
       "      <th>zip by</th>\n",
       "      <th>zombie apocalypse</th>\n",
       "      <th>zombie turned</th>\n",
       "      <th>zombie vampire</th>\n",
       "      <th>zombies anyway</th>\n",
       "      <th>zombies appeared</th>\n",
       "      <th>zombies on</th>\n",
       "      <th>zombies were</th>\n",
       "      <th>zoom up</th>\n",
       "      <th>î_ northside</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1525 rows × 64297 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00 am  00 for  00 in  00 meal  00 pm  00 sunday  000 mile  00am and  \\\n",
       "0         0       0      0        0      0          0         0         0   \n",
       "1         0       0      0        0      0          0         0         0   \n",
       "2         0       0      0        0      0          0         0         0   \n",
       "3         0       0      0        0      0          0         0         0   \n",
       "4         0       0      0        0      0          0         0         0   \n",
       "...     ...     ...    ...      ...    ...        ...       ...       ...   \n",
       "1520      0       0      0        0      0          0         0         0   \n",
       "1521      0       0      0        0      0          0         0         0   \n",
       "1522      0       0      0        0      0          0         0         0   \n",
       "1523      0       0      0        0      0          0         0         0   \n",
       "1524      0       0      0        0      0          0         0         0   \n",
       "\n",
       "      00am on  00am service  ...  zip by  zombie apocalypse  zombie turned  \\\n",
       "0           0             0  ...       0                  0              0   \n",
       "1           0             0  ...       0                  0              0   \n",
       "2           0             0  ...       0                  0              0   \n",
       "3           0             0  ...       0                  0              0   \n",
       "4           0             0  ...       0                  0              0   \n",
       "...       ...           ...  ...     ...                ...            ...   \n",
       "1520        0             0  ...       0                  0              0   \n",
       "1521        0             0  ...       0                  0              0   \n",
       "1522        0             0  ...       0                  0              0   \n",
       "1523        0             0  ...       0                  0              0   \n",
       "1524        0             0  ...       0                  0              0   \n",
       "\n",
       "      zombie vampire  zombies anyway  zombies appeared  zombies on  \\\n",
       "0                  0               0                 0           0   \n",
       "1                  0               0                 0           0   \n",
       "2                  0               0                 0           0   \n",
       "3                  0               0                 0           0   \n",
       "4                  0               0                 0           0   \n",
       "...              ...             ...               ...         ...   \n",
       "1520               0               0                 0           0   \n",
       "1521               0               0                 0           0   \n",
       "1522               0               0                 0           0   \n",
       "1523               0               0                 0           0   \n",
       "1524               0               0                 0           0   \n",
       "\n",
       "      zombies were  zoom up  î_ northside  \n",
       "0                0        0             0  \n",
       "1                0        0             0  \n",
       "2                0        0             0  \n",
       "3                0        0             0  \n",
       "4                0        0             0  \n",
       "...            ...      ...           ...  \n",
       "1520             0        0             0  \n",
       "1521             0        0             0  \n",
       "1522             0        0             0  \n",
       "1523             0        0             0  \n",
       "1524             0        0             0  \n",
       "\n",
       "[1525 rows x 64297 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(reviews_df[\"review\"])\n",
    "\n",
    "bigram_features = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "bigram_features.shape\n",
    "bigram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee31bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa84e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a6672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1887d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59690c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e5118e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195a479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1cfa1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c27d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7d93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5abc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce1b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b77148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9aa67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df9517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f22b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
