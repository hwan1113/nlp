{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes in Python\n",
    "The purpose of this notebook is to demonstrate a relatively simplistic implementation of Naive Bayes without the need for any ML libraries. We can simply use counts and lookup tables (ie. Python dictionaries) to fit our model and make inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (\"I ate dinner early\", \"HAM\"),\n",
    "    (\"free money today\", \"SPAM\"),\n",
    "    (\"I had a blast\", \"HAM\"),\n",
    "    (\"sign up free early today\", \"HAM\"),\n",
    "    (\"only free today\", \"SPAM\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I',\n",
       " 'a',\n",
       " 'ate',\n",
       " 'blast',\n",
       " 'dinner',\n",
       " 'early',\n",
       " 'free',\n",
       " 'had',\n",
       " 'money',\n",
       " 'only',\n",
       " 'sign',\n",
       " 'today',\n",
       " 'up'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = set()\n",
    "\n",
    "# Build corpus\n",
    "for document in documents:\n",
    "    text = document[0]\n",
    "    class_value = document[1]\n",
    "    for word in text.split():\n",
    "        corpus.add(word)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Conditional Probabilities\n",
    "We need to generate first $P(x|y)$. For instance, what is the likelihood of finding the word `free` if we know the document is `HAM` is represented as `P(x=\"free\"|y=\"HAM\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_probabilities = pd.DataFrame(index=list(corpus), \n",
    "                                         columns=[\"likelihood_given_ham\", \"likelihood_given_spam\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Our Priors\n",
    "We first want to obtain the values of $P(y =ham)$ and $P(y =spam)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ate dinner early, HAM\n",
      "Spam documents: 0\n",
      "Ham documents: 1 \n",
      "\n",
      "\n",
      "free money today, SPAM\n",
      "Spam documents: 1\n",
      "Ham documents: 1 \n",
      "\n",
      "\n",
      "I had a blast, HAM\n",
      "Spam documents: 1\n",
      "Ham documents: 2 \n",
      "\n",
      "\n",
      "sign up free early today, HAM\n",
      "Spam documents: 1\n",
      "Ham documents: 3 \n",
      "\n",
      "\n",
      "only free today, SPAM\n",
      "Spam documents: 2\n",
      "Ham documents: 3 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_documents = 0\n",
    "ham_documents = 0\n",
    "for doc, label in documents:\n",
    "    if label == \"SPAM\":\n",
    "        spam_documents += 1\n",
    "    else:\n",
    "        ham_documents += 1\n",
    "\n",
    "    print(f\"{doc}, {label}\")\n",
    "    print(f\"Spam documents: {spam_documents}\")\n",
    "    print(f\"Ham documents: {ham_documents} \\n\\n\")\n",
    "    \n",
    "p_ham = ham_documents / (spam_documents + ham_documents)\n",
    "p_spam = spam_documents / (spam_documents + ham_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Conditional Likelihoods\n",
    "We next want to compute the value of $P(x|y=ham)$ and $P(x|y=spam)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For word sign, 1 ham out of 3 ham documents.\n",
      "For word sign, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word early, 2 ham out of 3 ham documents.\n",
      "For word early, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word free, 1 ham out of 3 ham documents.\n",
      "For word free, 2 spam out of 2 spam documents.\n",
      "\n",
      "For word a, 1 ham out of 3 ham documents.\n",
      "For word a, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word I, 2 ham out of 3 ham documents.\n",
      "For word I, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word money, 0 ham out of 3 ham documents.\n",
      "For word money, 1 spam out of 2 spam documents.\n",
      "\n",
      "For word blast, 1 ham out of 3 ham documents.\n",
      "For word blast, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word ate, 1 ham out of 3 ham documents.\n",
      "For word ate, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word only, 0 ham out of 3 ham documents.\n",
      "For word only, 1 spam out of 2 spam documents.\n",
      "\n",
      "For word today, 1 ham out of 3 ham documents.\n",
      "For word today, 2 spam out of 2 spam documents.\n",
      "\n",
      "For word dinner, 1 ham out of 3 ham documents.\n",
      "For word dinner, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word had, 1 ham out of 3 ham documents.\n",
      "For word had, 0 spam out of 2 spam documents.\n",
      "\n",
      "For word up, 1 ham out of 3 ham documents.\n",
      "For word up, 0 spam out of 2 spam documents.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in corpus:\n",
    "    \n",
    "    ham_documents_with_word = 0\n",
    "    spam_documents_with_word = 0\n",
    "    \n",
    "    for document in documents:\n",
    "        document_class = document[1]\n",
    "        if word in document[0].split():\n",
    "            if document[1] == \"HAM\":\n",
    "                ham_documents_with_word += 1\n",
    "            else:\n",
    "                spam_documents_with_word += 1\n",
    "    \n",
    "    print(f\"For word {word}, {ham_documents_with_word} ham out of {ham_documents} ham documents.\")\n",
    "    print(f\"For word {word}, {spam_documents_with_word} spam out of {spam_documents} spam documents.\\n\")\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_ham\"] = ham_documents_with_word * 1.0 / ham_documents\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_spam\"] = spam_documents_with_word * 1.0 / spam_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likelihood_given_ham</th>\n",
       "      <th>likelihood_given_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sign</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blast</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dinner</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>up</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       likelihood_given_ham likelihood_given_spam\n",
       "sign               0.333333                   0.0\n",
       "early              0.666667                   0.0\n",
       "free               0.333333                   1.0\n",
       "a                  0.333333                   0.0\n",
       "I                  0.666667                   0.0\n",
       "money                   0.0                   0.5\n",
       "blast              0.333333                   0.0\n",
       "ate                0.333333                   0.0\n",
       "only                    0.0                   0.5\n",
       "today              0.333333                   1.0\n",
       "dinner             0.333333                   0.0\n",
       "had                0.333333                   0.0\n",
       "up                 0.333333                   0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Posterior Probablity of a Test Document\n",
    "Now that we have our priors and our likelihoods, we actually have everything we need to test out our Naive Bayes algorithm. We'll use a test document, `free today`, and compute its posterior probability $P(y=spam|x)$.\n",
    "\n",
    "Remember that using Bayes rule, we can rewrite this probability as \n",
    "\n",
    "$$\n",
    "P(y=spam|x) = \\frac{P(x|y=spam)P(y=spam)}{P(x)}\n",
    "$$\n",
    "The denominator, or the evidence, can be written as\n",
    "$$\n",
    "P(y=spam|x) = \\frac{P(x|y=spam)P(y=spam)}{P(x = \"free\"|y=spam)\\times P(x = \"today\" | y=spam)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = \"free today\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Function to Calculate the Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "def get_likelihood(test_document: str, conditional_probabilities: Dict)-> Tuple[float, float]:\n",
    "    likelihood_ham = 1\n",
    "    likelihood_spam = 1\n",
    "    for word in test_document.split():\n",
    "        likelihood_ham = likelihood_ham * conditional_probabilities.loc[word, \"likelihood_given_ham\"]\n",
    "        likelihood_spam = likelihood_spam * conditional_probabilities.loc[word, \"likelihood_given_spam\"]\n",
    "    \n",
    "    return likelihood_ham, likelihood_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_ham, likelihood_spam = get_likelihood(test_document, conditional_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Likelihoods and Priors to Calculate the Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(likelihood_ham: float, likelihood_spam: float, p_ham: float, p_spam: float)-> float:\n",
    "    posterior_ham = likelihood_ham * p_ham / (likelihood_ham * p_ham + likelihood_spam * p_spam)\n",
    "    posterior_spam = likelihood_spam * p_spam / (likelihood_ham * p_ham + likelihood_spam * p_spam)\n",
    "    return posterior_ham, posterior_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14285714285714285, 0.8571428571428572)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_posterior(likelihood_ham, likelihood_spam, p_ham, p_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the End to End Algorithm for Training a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes(documents):\n",
    "    corpus = set()\n",
    "    # Build corpus\n",
    "    for document in documents:\n",
    "        text = document[0]\n",
    "        class_value = document[1]\n",
    "        for word in text.split():\n",
    "            corpus.add(word)\n",
    "    \n",
    "    conditional_probabilities = pd.DataFrame(index=list(corpus), \n",
    "                                             columns=[\"likelihood_given_ham\", \"likelihood_given_spam\"])\n",
    "    \n",
    "    spam_documents = 0\n",
    "    ham_documents = 0\n",
    "    for document in documents:\n",
    "        if document[1] == \"SPAM\":\n",
    "            spam_documents += 1\n",
    "        else:\n",
    "            ham_documents += 1\n",
    "    p_ham = ham_documents / (spam_documents + ham_documents)\n",
    "    p_spam = spam_documents / (spam_documents + ham_documents)\n",
    "    \n",
    "    for word in corpus:\n",
    "        ham_documents_with_word = 0\n",
    "        spam_documents_with_word = 0\n",
    "    \n",
    "        for document in documents:\n",
    "            document_class = document[1]\n",
    "            if word in document[0].split():\n",
    "                if document[1] == \"HAM\":\n",
    "                    ham_documents_with_word += 1\n",
    "                else:\n",
    "                    spam_documents_with_word += 1\n",
    "\n",
    "        #print(f\"For word {word}, {ham_documents_with_word} ham out of {ham_documents}.\")\n",
    "        #print(f\"For word {word}, {spam_documents_with_word} spam out of {spam_documents}.\")\n",
    "        conditional_probabilities.loc[word, \"likelihood_given_ham\"] = ham_documents_with_word * 1.0 / ham_documents\n",
    "        conditional_probabilities.loc[word, \"likelihood_given_spam\"] = spam_documents_with_word * 1.0 / spam_documents\n",
    "\n",
    "    \n",
    "    return conditional_probabilities, p_ham, p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       likelihood_given_ham likelihood_given_spam\n",
       " sign               0.333333                   0.0\n",
       " early              0.666667                   0.0\n",
       " free               0.333333                   1.0\n",
       " a                  0.333333                   0.0\n",
       " I                  0.666667                   0.0\n",
       " money                   0.0                   0.5\n",
       " blast              0.333333                   0.0\n",
       " ate                0.333333                   0.0\n",
       " only                    0.0                   0.5\n",
       " today              0.333333                   1.0\n",
       " dinner             0.333333                   0.0\n",
       " had                0.333333                   0.0\n",
       " up                 0.333333                   0.0,\n",
       " 0.6,\n",
       " 0.4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_naive_bayes(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Non-Existent Words\n",
    "\n",
    "From [Sebastian Raschka, Python Machine Learning](https://arxiv.org/pdf/1410.5329.pdf)\n",
    "![Correlations](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/smoothing.png \"Visualization of various r values for Pearson correlation coefficient\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
