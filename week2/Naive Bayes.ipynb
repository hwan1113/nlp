{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes in Python\n",
    "The purpose of this notebook is to demonstrate a relatively simplistic implementation of Naive Bayes without the need for any ML libraries. We can simply use counts and lookup tables (ie. Python dictionaries) to fit our model and make inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (\"I ate dinner early\", \"HAM\"),\n",
    "    (\"free money today\", \"SPAM\"),\n",
    "    (\"I had a blast\", \"HAM\"),\n",
    "    (\"sign up free early today\", \"HAM\"),\n",
    "    (\"only free today\", \"SPAM\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = set()\n",
    "\n",
    "# Build corpus\n",
    "for document in documents:\n",
    "    text = document[0]\n",
    "    class_value = document[1]\n",
    "    for word in text.split():\n",
    "        corpus.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Conditional Probabilities\n",
    "We need to generate first $P(x|y)$. For instance, what is the likelihood of finding the word `free` if we know the document is `HAM` is represented as `P(x=\"free\"|y=\"HAM\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_probabilities = pd.DataFrame(index=list(corpus), \n",
    "                                         columns=[\"likelihood_given_ham\", \"likelihood_given_spam\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Our Priors\n",
    "We first want to obtain the values of $P(y =ham)$ and $P(y =spam)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_documents = 0\n",
    "ham_documents = 0\n",
    "for doc, label in documents:\n",
    "    if label == \"SPAM\":\n",
    "        spam_documents += 1\n",
    "    else:\n",
    "        ham_documents += 1\n",
    "\n",
    "    print(f\"{doc}, {label}\")\n",
    "    print(f\"Spam documents: {spam_documents}\")\n",
    "    print(f\"Ham documents: {ham_documents} \\n\\n\")\n",
    "    \n",
    "p_ham = ham_documents / (spam_documents + ham_documents)\n",
    "p_spam = spam_documents / (spam_documents + ham_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Conditional Likelihoods\n",
    "We next want to compute the value of $P(x|y=ham)$ and $P(x|y=spam)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in corpus:\n",
    "    \n",
    "    ham_documents_with_word = 0\n",
    "    spam_documents_with_word = 0\n",
    "    \n",
    "    for document in documents:\n",
    "        document_class = document[1]\n",
    "        if word in document[0].split():\n",
    "            if document[1] == \"HAM\":\n",
    "                ham_documents_with_word += 1\n",
    "            else:\n",
    "                spam_documents_with_word += 1\n",
    "    \n",
    "    print(f\"For word {word}, {ham_documents_with_word} ham out of {ham_documents} ham documents.\")\n",
    "    print(f\"For word {word}, {spam_documents_with_word} spam out of {spam_documents} spam documents.\\n\")\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_ham\"] = ham_documents_with_word * 1.0 / ham_documents\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_spam\"] = spam_documents_with_word * 1.0 / spam_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Posterior Probablity of a Test Document\n",
    "Now that we have our priors and our likelihoods, we actually have everything we need to test out our Naive Bayes algorithm. We'll use a test document, `free today`, and compute its posterior probability $P(y=spam|x)$.\n",
    "\n",
    "Remember that using Bayes rule, we can rewrite this probability as \n",
    "\n",
    "$$\n",
    "P(y=spam|x) = \\frac{P(x|y=spam)P(y=spam)}{P(x)}\n",
    "$$\n",
    "The denominator, or the evidence, can be written as\n",
    "$$\n",
    "P(y=spam|x) = \\frac{P(x|y=spam)P(y=spam)}{P(x = \"free\"|y=spam)\\times P(x = \"today\" | y=spam)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = \"free today\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Function to Calculate the Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "def get_likelihood(test_document: str, conditional_probabilities: Dict)-> Tuple[float, float]:\n",
    "    likelihood_ham = 1\n",
    "    likelihood_spam = 1\n",
    "    for word in test_document.split():\n",
    "        likelihood_ham = likelihood_ham * conditional_probabilities.loc[word, \"likelihood_given_ham\"]\n",
    "        likelihood_spam = likelihood_spam * conditional_probabilities.loc[word, \"likelihood_given_spam\"]\n",
    "    \n",
    "    return likelihood_ham, likelihood_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_ham, likelihood_spam = get_likelihood(test_document, conditional_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Likelihoods and Priors to Calculate the Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(likelihood_ham: float, likelihood_spam: float, p_ham: float, p_spam: float)-> float:\n",
    "    posterior_ham = likelihood_ham * p_ham / (likelihood_ham * p_ham + likelihood_spam * p_spam)\n",
    "    posterior_spam = likelihood_spam * p_spam / (likelihood_ham * p_ham + likelihood_spam * p_spam)\n",
    "    return posterior_ham, posterior_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_posterior(likelihood_ham, likelihood_spam, p_ham, p_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the End to End Algorithm for Training a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes(documents):\n",
    "    corpus = set()\n",
    "    # Build corpus\n",
    "    for document in documents:\n",
    "        text = document[0]\n",
    "        class_value = document[1]\n",
    "        for word in text.split():\n",
    "            corpus.add(word)\n",
    "    \n",
    "    conditional_probabilities = pd.DataFrame(index=list(corpus), \n",
    "                                             columns=[\"likelihood_given_ham\", \"likelihood_given_spam\"])\n",
    "    \n",
    "    spam_documents = 0\n",
    "    ham_documents = 0\n",
    "    for document in documents:\n",
    "        if document[1] == \"SPAM\":\n",
    "            spam_documents += 1\n",
    "        else:\n",
    "            ham_documents += 1\n",
    "    p_ham = ham_documents / (spam_documents + ham_documents)\n",
    "    p_spam = spam_documents / (spam_documents + ham_documents)\n",
    "    \n",
    "    for word in corpus:\n",
    "        ham_documents_with_word = 0\n",
    "        spam_documents_with_word = 0\n",
    "    \n",
    "        for document in documents:\n",
    "            document_class = document[1]\n",
    "            if word in document[0].split():\n",
    "                if document[1] == \"HAM\":\n",
    "                    ham_documents_with_word += 1\n",
    "                else:\n",
    "                    spam_documents_with_word += 1\n",
    "\n",
    "        #print(f\"For word {word}, {ham_documents_with_word} ham out of {ham_documents}.\")\n",
    "        #print(f\"For word {word}, {spam_documents_with_word} spam out of {spam_documents}.\")\n",
    "        conditional_probabilities.loc[word, \"likelihood_given_ham\"] = ham_documents_with_word * 1.0 / ham_documents\n",
    "        conditional_probabilities.loc[word, \"likelihood_given_spam\"] = spam_documents_with_word * 1.0 / spam_documents\n",
    "\n",
    "    \n",
    "    return conditional_probabilities, p_ham, p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_naive_bayes(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Non-Existent Words\n",
    "\n",
    "From [Sebastian Raschka, Python Machine Learning](https://arxiv.org/pdf/1410.5329.pdf)\n",
    "![Correlations](https://raw.githubusercontent.com/ychennay/dso-560-nlp-text-analytics/main/images/smoothing.png \"Visualization of various r values for Pearson correlation coefficient\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
