{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjElzEXdaO9-"
   },
   "source": [
    "### What is Textacy?\n",
    "\n",
    "[textacy](https://github.com/chartbeat-labs/textacy) is a text pre/post-processing framework that will help make many of the tasks we performed in this course significantly easier. As its [Github description](https://github.com/chartbeat-labs/textacy) states:\n",
    "> *textacy is a Python library for performing a variety of natural language processing (NLP) tasks, built on the high-performance spaCy library. With the fundamentals --- tokenization, part-of-speech tagging, dependency parsing, etc. --- delegated to another library, textacy focuses primarily on the tasks that come before and follow after.*\n",
    "\n",
    "While `spacy` focuses on tokenization, part of speech tagging, named entity recognition, etc., `textacy` focuses on all the different tasks that come before and after.\n",
    "\n",
    "Check out the [Textacy documentation](https://textacy.readthedocs.io/en/0.11.0/quickstart.html#) for all the different use cases you can apply `textacy` to - only a few common ones are shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q43BRs2Tv7Fz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textacy in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (0.12.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (1.7.1)\n",
      "Requirement already satisfied: jellyfish>=0.8.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (4.62.3)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (0.11.0)\n",
      "Requirement already satisfied: cachetools>=4.0.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (5.0.0)\n",
      "Requirement already satisfied: spacy>=3.0.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (3.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (0.24.2)\n",
      "Requirement already satisfied: pyphen>=0.10.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (0.12.0)\n",
      "Requirement already satisfied: catalogue~=2.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (2.0.7)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (1.1.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from textacy) (2.6.3)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from cytoolz>=0.10.1->textacy) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->textacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->textacy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->textacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->textacy) (3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.19.0->textacy) (2.2.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (0.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (0.6.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (2.11.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (1.0.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (0.9.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (8.0.15)\n",
      "Requirement already satisfied: setuptools in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (58.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (0.7.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=3.0.0->textacy) (3.10.0.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy>=3.0.0->textacy) (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/Users/hwan/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install library\n",
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6e655U5aV0D"
   },
   "source": [
    "### Import Data\n",
    "We will import the `SMS_train.csv` dataset from week 4 homework to use as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxKtTwpCaTBv",
    "outputId": "3d5a0706-7cb3-4cca-d1f4-8b559d178ef0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sms_df = pd.read_csv(\"../datasets/SMS_train.csv\", encoding=\"latin1\")\n",
    "sms_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS0m1HOTbn90"
   },
   "source": [
    "#### Grouping Concepts\n",
    "\n",
    "One of the attributes of this dataset is the presence of URLs. Textacy has already defined regex to parse out URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPNLfPvda6O3",
    "outputId": "0e179fed-350f-4d9c-b76d-36d319b17796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex for URLs: re.compile('(?:^|(?<![\\\\w/.]))(?:(?:https?://|ftp://|www\\\\d{0,3}\\\\.))(?:\\\\S+(?::\\\\S*)?@)?(?:(?!(?:10|127)(?:\\\\.\\\\d{1,3}){3})(?!(?:169\\\\.254|192\\\\.168)(?:\\\\.\\\\d{1,3}){2})(?!172\\\\.(?:1[6-9]|2\\\\d|3[0-1])(?:\\\\.\\\\d{1, re.IGNORECASE)\n",
      "Regex for short URLs: re.compile('(?:^|(?<![\\\\w/.]))(?:(?:https?://)?)(?:\\\\w-?)*?\\\\w+(?:\\\\.[a-z]{2,12}){1,3}/[^\\\\s.,?!\\'\\\\\"|+]{2,12}(?:$|(?![\\\\w?!+&/]))', re.IGNORECASE)\n",
      "Found the following URLs: ['www.comuk.net', 'www.gamb.tv', 'www.shortbreaks.org.uk', 'www.dbuk.net', 'www.t-c.biz', 'www.SMS.ac/u/nat27081980', 'www.telediscount.co.uk', 'www.getzed.co.uk', 'www.ringtones.co.uk', 'www.SMS.ac/u/natalie2k9', 'www.SMS.ac/u/goldviking', 'www.SMS.ac/u/hmmross', 'www.4-tc.biz', 'www.santacalling.com', 'www.fullonsms.com', 'www.cashbin.co.uk', 'www.win-82050.co.uk', 'www.clubmoby.com']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import itertools\n",
    "from textacy.preprocessing.resources import RE_URL\n",
    "from textacy.preprocessing.resources import RE_SHORT_URL\n",
    "print(f\"Regex for URLs: {RE_URL}\")\n",
    "print(f\"Regex for short URLs: {RE_SHORT_URL}\")\n",
    "results: List[List[str]] = sms_df.Message_body.str.findall(RE_URL).tolist()\n",
    "\n",
    "parsed_urls: List[str] = list(itertools.chain(*results))\n",
    "print(f\"Found the following URLs: {parsed_urls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.comuk.net'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.gamb.tv'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.shortbreaks.org.uk'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.dbuk.net'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.t-c.biz'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.SMS.ac/u/nat27081980'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.telediscount.co.uk'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.getzed.co.uk'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.ringtones.co.uk'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.SMS.ac/u/natalie2k9'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.SMS.ac/u/goldviking'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.SMS.ac/u/hmmross'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.4-tc.biz'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.santacalling.com'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.fullonsms.com'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.cashbin.co.uk'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.win-82050.co.uk'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['www.clubmoby.com'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPKYZ2v_dU0M"
   },
   "source": [
    "We can quickly replace all of these URLs with a predefined tagged token, like `_URL_` by using the `replace_urls` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3ITF6LOTjF96",
    "outputId": "aa8a9bb9-c09a-45c5-f234-d0d36071d1a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a url: _URL_'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy.preprocessing.replace import urls\n",
    "text = \"This is a url: http://www.google.com\"\n",
    "urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-VxdCtKbr8S",
    "outputId": "cf62b0b3-92f3-40ac-fad1-d114c99441a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Rofl. Its true to its name\n",
       "1    The guy did some bitching but I acted like i'd...\n",
       "2    Pity, * was in mood for that. So...any other s...\n",
       "3                 Will ü b going to esplanade fr home?\n",
       "4    This is the 2nd time we have tried 2 contact u...\n",
       "Name: Message_body, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.Message_body.apply(urls)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPSKq_sTlzZ6"
   },
   "source": [
    "We can replace all sorts of different entities/concepts, such as URLs, hashtags, numbers, emails, etc.\n",
    "\n",
    "We can also use the regex defined by `textacy`. Below we define a pipeline to find and replace common entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UohDlYzXdf-C",
    "outputId": "3d7ff9a5-a12f-4cc3-bd4d-cca49fb0ae0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Rofl. Its true to its name\n",
       "1    The guy did some bitching but I acted like i'd...\n",
       "2    Pity, * was in mood for that. So...any other s...\n",
       "3                 Will ü b going to esplanade fr home?\n",
       "4    This is the 2nd time we have tried _NUMBER_ co...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy.preprocessing.replace import urls, hashtags, numbers, emails, emojis, currency_symbols\n",
    "sms_df[\"cleaned_text\"] = sms_df.Message_body.\\\n",
    "  apply(urls).\\\n",
    "  apply(hashtags).\\\n",
    "  apply(numbers).\\\n",
    "  apply(currency_symbols).\\\n",
    "  apply(emojis).\\\n",
    "  apply(emails)\n",
    "sms_df.cleaned_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjVspJFHmbHx"
   },
   "source": [
    "We can also use `textacy` to remove or normalized undesired text elements. For instance, there are often many different manifestations of quotation marks and bullet points, especially if you are dealing with text that is formatted from a word processor like Microsoft Word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kgilj4rznEmR",
    "outputId": "0fa3bebf-50c5-40f8-d349-e213fbc42827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before counts: Counter({'\"': 1, '“': 1, '”': 1})\n",
      "After counts: Counter({'\"': 3})\n",
      "Before counts: Counter({'•': 1, '‣': 1, '⁃': 1, '-': 1})\n",
      "Before counts: Counter({'-': 4})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from textacy.preprocessing.normalize import quotation_marks, bullet_points\n",
    "quotes = ['\"','“','”']\n",
    "print(f\"Before counts: {Counter(quotes)}\")\n",
    "print(f\"After counts: {Counter(map(quotation_marks, quotes))}\")\n",
    "\n",
    "points = [\"•\", \"‣\", \"⁃\", \"-\"]\n",
    "print(f\"Before counts: {Counter(points)}\")\n",
    "print(f\"Before counts: {Counter(map(bullet_points, points))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJJTce3Lo2Ow"
   },
   "source": [
    "A common text preprocessing task we performed in this course is removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gF0ComCrnU1G",
    "outputId": "572d6940-3b2f-41d3-c21f-19b95b926fb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Rofl  Its true to its name\n",
       "1    The guy did some bitching but I acted like i d...\n",
       "2    Pity    was in mood for that  So   any other s...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy.preprocessing.remove import punctuation\n",
    "sms_df.cleaned_text[:3].apply(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Bnmvij0pUSk"
   },
   "source": [
    "### Text Extraction\n",
    "\n",
    "You can also use `textacy` to extract ngrams, named entities, and even key terms from a piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2Y_ViMIvqYx3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: jinja2 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/hwan/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/Users/hwan/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "50qEggFhp11A"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"\"\"\n",
    "I am eating dinner at the restaurant on Main Street, the best eatery this side of New York City. \n",
    "He went running down the street, but could not find his bike.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "veUW_C7CpTzP",
    "outputId": "8d80d477-8213-4c96-e16d-632f00bfa776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-grams with stopwords: [I am, am eating, eating dinner, dinner at, at the, the restaurant, restaurant on, on Main, Main Street, the best, best eatery, eatery this, this side, side of, of New, New York, York City, He went, went running, running down, down the, the street, but could, could not, not find, find his, his bike]\n",
      "n-grams without stopwords: [eating dinner, Main Street, best eatery, New York, York City, went running]\n"
     ]
    }
   ],
   "source": [
    "from textacy import extract\n",
    "# note that you must pass in a spacy Doc, not a string\n",
    "print(f\"n-grams with stopwords: {list(extract.ngrams(doc, n=2, filter_stops=False))}\")\n",
    "print(f\"n-grams without stopwords: {list(extract.ngrams(doc, n=2, filter_stops=True))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zSPBm2_pFD3",
    "outputId": "1559262a-7749-4701-fb51-ecdfd105ac20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named entities: [Main Street, New York City]\n"
     ]
    }
   ],
   "source": [
    "print(f\"named entities: {list(extract.entities(doc))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cCjACFWsS7y"
   },
   "source": [
    "### Parsing Key Terms\n",
    "`textacy` also can attempt to parse out what it believes are key words from a particular document. There are a variety of algorithms it can use:\n",
    "\n",
    "* [TextRank](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "* [SGRank](https://aclanthology.org/S15-1013.pdf)\n",
    "* [YAKE](https://github.com/LIAAD/yake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qxW1O38rtKU",
    "outputId": "3d289268-57b4-4e1f-d2a3-4928431d1a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key terms: [('New York City', 0.08906773052656537), ('good eatery', 0.05593421627154432), ('Main Street', 0.05483321797094359), ('bike', 0.028799215152480313), ('dinner', 0.0285773930627672), ('restaurant', 0.026648908092068536), ('street', 0.02508976848714809)]\n",
      "key terms w/ window size = 4: [('New York City', 0.08858588611075899), ('good eatery', 0.05758818253165755), ('Main Street', 0.05327412352235519), ('dinner', 0.029984215962700136), ('restaurant', 0.029182573041746988), ('street', 0.028744547498104688), ('bike', 0.021793912595648182)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"key terms: {list(extract.keyterms.textrank(doc))}\")\n",
    "print(f\"key terms w/ window size = 4: {list(extract.keyterms.textrank(doc, window_size=4))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_pJUGWGsNQM",
    "outputId": "87d0030c-c3d8-436a-8634-6d5e48962417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key terms: [('New York City', 0.3517458042211082), ('good eatery', 0.2112484604816762), ('Main Street', 0.15856049498256797), ('restaurant', 0.08132137758569719), ('street', 0.06737092215444981), ('bike', 0.06561240483667229), ('dinner', 0.06414053573782827)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"key terms: {list(extract.keyterms.sgrank(doc))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyDKOCEjH6uD",
    "outputId": "10198dd7-0bc6-4f6d-c3a7-eb13b0fe113e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key terms: [('New York City', 0.333801710490245), ('Main Street', 0.44164399917429203), ('bike', 0.7774388474035969), ('good', 0.8049257265599533), ('dinner', 0.8392874245523302), ('restaurant', 0.8392874245523302), ('eatery', 0.8392874245523302), ('street', 0.8613045009868965), ('good eatery', 2.08227238435987)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"key terms: {list(extract.keyterms.yake(doc))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQpFu9zpIqQf"
   },
   "source": [
    "### Generating Text Statistics\n",
    "\n",
    "You can often summarize a corpus and examine its properties to determine how similar one corpus is to another corpus. [Textacy has a number of functions to help parse out these properties/statistics](https://textacy.readthedocs.io/en/0.11.0/api_reference/text_stats.html#textacy.text_stats.readability.gunning_fog_index). This can be useful for identifying authorship or source when you are not certain where certain text originated from, or if you wish to cluster text together using an unsupervised clustering algorithm such as **K-Nearest Neighbors**.\n",
    "\n",
    "Common useful stats (definitions directly from [Textacy documentation](https://textacy.readthedocs.io/en/0.11.0/api_reference/text_stats.html)):\n",
    "- **[Flesch Reading Ease](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch.E2.80.93Kincaid_grade_level)**: Readability test used as a general-purpose standard in several languages, based on a weighted combination of avg. sentence length and avg. word length. Values usually fall in the range [0, 100], but may be arbitrarily negative in extreme cases. Higher value => easier text.\n",
    "- **[Gunning Fog Index](https://en.wikipedia.org/wiki/Gunning_fog_index)**: Readability test commonly used in Sweden on both English- and non-English-language texts, whose value estimates the difficulty of reading a foreign text. Higher value => more difficult text.\n",
    "- **[Smog Index](https://en.wikipedia.org/wiki/SMOG)**: Readability test commonly used in medical writing and the healthcare industry, whose value estimates the number of years of education required to understand a text similar to `flesch_kincaid_grade_level()` and intended as a substitute for `gunning_fog_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVGB2jYgH8-j",
    "outputId": "be7df341-9c96-47a1-f6d5-acc54928c4e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hwan/opt/anaconda3/lib/python3.9/site-packages/textacy/text_stats/api.py:89: DeprecationWarning: The `TextStats` class is deprecated as of v0.12. Instead, call the stats functions directly -- `text_stats.TextStats(doc).n_sents` => `text_stats.n_sents(doc)` --or set them as custom doc extensions and access them on the ``Doc`` -- `textacy.set_doc_extensions('text_stats'); doc._.n_sents` .\n",
      "  utils.deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 6.345230909424329\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextStats' object has no attribute 'flesch_kincaid_grade_level'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lr/7v936_cn425434l_8lls3p4m0000gn/T/ipykernel_5890/1186291800.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Entropy: {ts.entropy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Flesch Grade Level: {ts.flesch_kincaid_grade_level}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Smog Index: {ts.smog_index}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextStats' object has no attribute 'flesch_kincaid_grade_level'"
     ]
    }
   ],
   "source": [
    "from textacy.text_stats import TextStats\n",
    "from textacy import make_spacy_doc\n",
    "doc = make_spacy_doc(\"\"\"\n",
    "A month ago, new coronavirus cases in the United States were ticking steadily \n",
    "downward and the worst of a miserable summer surge fueled by the Delta variant \n",
    "appeared to be over. But as Americans travel this week to meet far-flung \n",
    "relatives for Thanksgiving dinner, new virus cases are rising once more, \n",
    "especially in the Upper Midwest and Northeast.\n",
    "\n",
    "Federal medical teams have been dispatched to Minnesota to help at overwhelmed \n",
    "hospitals. Michigan is enduring its worst case surge yet, with daily caseloads \n",
    "doubling since the start of November. Even New England, where vaccination rates \n",
    "are high, is struggling, with Vermont, Maine and New Hampshire trying to \n",
    "contain major outbreaks.\n",
    "\"\"\",  lang=\"en_core_web_sm\")\n",
    "ts = TextStats(doc)\n",
    "print(f\"Entropy: {ts.entropy}\")\n",
    "print(f\"Flesch Grade Level: {ts.flesch_kincaid_grade_level}\")\n",
    "print(f\"Smog Index: {ts.smog_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPUK2xmrJJ1q",
    "outputId": "65f6a0e5-a244-45ed-e8f7-465beeec0620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 1.584962500721156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hwan/opt/anaconda3/lib/python3.9/site-packages/textacy/text_stats/api.py:89: DeprecationWarning: The `TextStats` class is deprecated as of v0.12. Instead, call the stats functions directly -- `text_stats.TextStats(doc).n_sents` => `text_stats.n_sents(doc)` --or set them as custom doc extensions and access them on the ``Doc`` -- `textacy.set_doc_extensions('text_stats'); doc._.n_sents` .\n",
      "  utils.deprecated(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextStats' object has no attribute 'flesch_kincaid_grade_level'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lr/7v936_cn425434l_8lls3p4m0000gn/T/ipykernel_5890/3865430633.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Entropy: {ts.entropy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Flesch Grade Level: {ts.flesch_kincaid_grade_level}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Smog Index: {ts.smog_index}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextStats' object has no attribute 'flesch_kincaid_grade_level'"
     ]
    }
   ],
   "source": [
    "doc = make_spacy_doc(\"\"\"\n",
    "He do good.\n",
    "\"\"\",  lang=\"en_core_web_sm\")\n",
    "ts = TextStats(doc)\n",
    "print(f\"Entropy: {ts.entropy}\")\n",
    "print(f\"Flesch Grade Level: {ts.flesch_kincaid_grade_level}\")\n",
    "print(f\"Smog Index: {ts.smog_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxps-1VSKGD8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Using Textacy for Text Pre-Processing",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
