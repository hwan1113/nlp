{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjElzEXdaO9-"
   },
   "source": [
    "### What is Textacy?\n",
    "\n",
    "[textacy](https://github.com/chartbeat-labs/textacy) is a text pre/post-processing framework that will help make many of the tasks we performed in this course significantly easier. As its [Github description](https://github.com/chartbeat-labs/textacy) states:\n",
    "> *textacy is a Python library for performing a variety of natural language processing (NLP) tasks, built on the high-performance spaCy library. With the fundamentals --- tokenization, part-of-speech tagging, dependency parsing, etc. --- delegated to another library, textacy focuses primarily on the tasks that come before and follow after.*\n",
    "\n",
    "While `spacy` focuses on tokenization, part of speech tagging, named entity recognition, etc., `textacy` focuses on all the different tasks that come before and after.\n",
    "\n",
    "Check out the [Textacy documentation](https://textacy.readthedocs.io/en/0.11.0/quickstart.html#) for all the different use cases you can apply `textacy` to - only a few common ones are shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q43BRs2Tv7Fz"
   },
   "outputs": [],
   "source": [
    "# install library\n",
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6e655U5aV0D"
   },
   "source": [
    "### Import Data\n",
    "We will import the `SMS_train.csv` dataset from week 4 homework to use as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxKtTwpCaTBv",
    "outputId": "3d5a0706-7cb3-4cca-d1f4-8b559d178ef0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(957, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sms_df = pd.read_csv(\"../datasets/SMS_train.csv\", encoding=\"latin1\")\n",
    "sms_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS0m1HOTbn90"
   },
   "source": [
    "#### Grouping Concepts\n",
    "\n",
    "One of the attributes of this dataset is the presence of URLs. Textacy has already defined regex to parse out URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPNLfPvda6O3",
    "outputId": "0e179fed-350f-4d9c-b76d-36d319b17796"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import itertools\n",
    "from textacy.preprocessing.resources import RE_URL\n",
    "from textacy.preprocessing.resources import RE_SHORT_URL\n",
    "print(f\"Regex for URLs: {RE_URL}\")\n",
    "print(f\"Regex for short URLs: {RE_SHORT_URL}\")\n",
    "results: List[List[str]] = sms_df.Message_body.str.findall(RE_URL).tolist()\n",
    "\n",
    "parsed_urls: List[str] = list(itertools.chain(*results))\n",
    "print(f\"Found the following URLs: {parsed_urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPKYZ2v_dU0M"
   },
   "source": [
    "We can quickly replace all of these URLs with a predefined tagged token, like `_URL_` by using the `replace_urls` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3ITF6LOTjF96",
    "outputId": "aa8a9bb9-c09a-45c5-f234-d0d36071d1a0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'This is a url: _URL_'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy.preprocessing.replace import urls\n",
    "text = \"This is a url: http://www.google.com\"\n",
    "urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-VxdCtKbr8S",
    "outputId": "cf62b0b3-92f3-40ac-fad1-d114c99441a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Rofl. Its true to its name\n",
       "1    The guy did some bitching but I acted like i'd...\n",
       "2    Pity, * was in mood for that. So...any other s...\n",
       "3                 Will ü b going to esplanade fr home?\n",
       "4    This is the 2nd time we have tried 2 contact u...\n",
       "Name: Message_body, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.Message_body.apply(urls)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPSKq_sTlzZ6"
   },
   "source": [
    "We can replace all sorts of different entities/concepts, such as URLs, hashtags, numbers, emails, etc.\n",
    "\n",
    "We can also use the regex defined by `textacy`. Below we define a pipeline to find and replace common entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UohDlYzXdf-C",
    "outputId": "3d7ff9a5-a12f-4cc3-bd4d-cca49fb0ae0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Rofl. Its true to its name\n",
       "1    The guy did some bitching but I acted like i'd...\n",
       "2    Pity, * was in mood for that. So...any other s...\n",
       "3                 Will ü b going to esplanade fr home?\n",
       "4    This is the 2nd time we have tried _NUMBER_ co...\n",
       "Name: Message_body, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy.preprocessing.replace import urls, hashtags, numbers, emails, emojis, currency_symbols\n",
    "sms_df[\"cleaned_text\"] = sms_df.Message_body.\\\n",
    "  apply(urls).\\\n",
    "  apply(hashtags).\\\n",
    "  apply(numbers).\\\n",
    "  apply(currency_symbols).\\\n",
    "  apply(emojis).\\\n",
    "  apply(emails)\n",
    "sms_df.cleaned_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjVspJFHmbHx"
   },
   "source": [
    "We can also use `textacy` to remove or normalized undesired text elements. For instance, there are often many different manifestations of quotation marks and bullet points, especially if you are dealing with text that is formatted from a word processor like Microsoft Word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kgilj4rznEmR",
    "outputId": "0fa3bebf-50c5-40f8-d349-e213fbc42827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before counts: Counter({'\"': 1, '“': 1, '”': 1})\n",
      "After counts: Counter({'\"': 3})\n",
      "Before counts: Counter({'•': 1, '‣': 1, '⁃': 1, '-': 1})\n",
      "Before counts: Counter({'-': 4})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from textacy.preprocessing.normalize import quotation_marks, bullet_points\n",
    "quotes = ['\"','“','”']\n",
    "print(f\"Before counts: {Counter(quotes)}\")\n",
    "print(f\"After counts: {Counter(map(quotation_marks, quotes))}\")\n",
    "\n",
    "points = [\"•\", \"‣\", \"⁃\", \"-\"]\n",
    "print(f\"Before counts: {Counter(points)}\")\n",
    "print(f\"Before counts: {Counter(map(bullet_points, points))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJJTce3Lo2Ow"
   },
   "source": [
    "A common text preprocessing task we performed in this course is removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gF0ComCrnU1G",
    "outputId": "572d6940-3b2f-41d3-c21f-19b95b926fb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Rofl  Its true to its name\n",
       "1    The guy did some bitching but I acted like i d...\n",
       "2    Pity    was in mood for that  So   any other s...\n",
       "Name: Message_body, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy.preprocessing.remove import punctuation\n",
    "sms_df.cleaned_text[:3].apply(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Bnmvij0pUSk"
   },
   "source": [
    "### Text Extraction\n",
    "\n",
    "You can also use `textacy` to extract ngrams, named entities, and even key terms from a piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Y_ViMIvqYx3"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50qEggFhp11A"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"\"\"\n",
    "I am eating dinner at the restaurant on Main Street, the best eatery this side of New York City. \n",
    "He went running down the street, but could not find his bike.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "veUW_C7CpTzP",
    "outputId": "8d80d477-8213-4c96-e16d-632f00bfa776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-grams with stopwords: [I am, am eating, eating dinner, dinner at, at the, the restaurant, restaurant on, on Main, Main Street, the best, best eatery, eatery this, this side, side of, of New, New York, York City, He went, went running, running down, down the, the street, but could, could not, not find, find his, his bike]\n",
      "n-grams without stopwords: [eating dinner, Main Street, best eatery, New York, York City, went running]\n"
     ]
    }
   ],
   "source": [
    "from textacy import extract\n",
    "# note that you must pass in a spacy Doc, not a string\n",
    "print(f\"n-grams with stopwords: {list(extract.ngrams(doc, n=2, filter_stops=False))}\")\n",
    "print(f\"n-grams without stopwords: {list(extract.ngrams(doc, n=2, filter_stops=True))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zSPBm2_pFD3",
    "outputId": "1559262a-7749-4701-fb51-ecdfd105ac20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named entities: [Main Street, New York City]\n"
     ]
    }
   ],
   "source": [
    "print(f\"named entities: {list(extract.entities(doc))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cCjACFWsS7y"
   },
   "source": [
    "### Parsing Key Terms\n",
    "`textacy` also can attempt to parse out what it believes are key words from a particular document. There are a variety of algorithms it can use:\n",
    "\n",
    "* [TextRank](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "* [SGRank](https://aclanthology.org/S15-1013.pdf)\n",
    "* [YAKE](https://github.com/LIAAD/yake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qxW1O38rtKU",
    "outputId": "3d289268-57b4-4e1f-d2a3-4928431d1a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key terms: [('New York City', 0.08906773052656537), ('good eatery', 0.05593421627154432), ('Main Street', 0.05483321797094359), ('bike', 0.028799215152480313), ('dinner', 0.0285773930627672), ('restaurant', 0.026648908092068536), ('street', 0.02508976848714809)]\n",
      "key terms w/ window size = 4: [('New York City', 0.08858588611075899), ('good eatery', 0.05758818253165755), ('Main Street', 0.05327412352235519), ('dinner', 0.029984215962700136), ('restaurant', 0.029182573041746988), ('street', 0.028744547498104688), ('bike', 0.021793912595648182)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"key terms: {list(extract.keyterms.textrank(doc))}\")\n",
    "print(f\"key terms w/ window size = 4: {list(extract.keyterms.textrank(doc, window_size=4))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_pJUGWGsNQM",
    "outputId": "87d0030c-c3d8-436a-8634-6d5e48962417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key terms: [('New York City', 0.3517458042211082), ('good eatery', 0.2112484604816762), ('Main Street', 0.15856049498256797), ('restaurant', 0.08132137758569719), ('street', 0.06737092215444981), ('bike', 0.06561240483667229), ('dinner', 0.06414053573782827)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"key terms: {list(extract.keyterms.sgrank(doc))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyDKOCEjH6uD",
    "outputId": "10198dd7-0bc6-4f6d-c3a7-eb13b0fe113e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key terms: [('New York City', 0.333801710490245), ('Main Street', 0.44164399917429203), ('bike', 0.7774388474035969), ('good', 0.8049257265599533), ('dinner', 0.8392874245523302), ('restaurant', 0.8392874245523302), ('eatery', 0.8392874245523302), ('street', 0.8613045009868965), ('good eatery', 2.08227238435987)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"key terms: {list(extract.keyterms.yake(doc))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQpFu9zpIqQf"
   },
   "source": [
    "### Generating Text Statistics\n",
    "\n",
    "You can often summarize a corpus and examine its properties to determine how similar one corpus is to another corpus. [Textacy has a number of functions to help parse out these properties/statistics](https://textacy.readthedocs.io/en/0.11.0/api_reference/text_stats.html#textacy.text_stats.readability.gunning_fog_index). This can be useful for identifying authorship or source when you are not certain where certain text originated from, or if you wish to cluster text together using an unsupervised clustering algorithm such as **K-Nearest Neighbors**.\n",
    "\n",
    "Common useful stats (definitions directly from [Textacy documentation](https://textacy.readthedocs.io/en/0.11.0/api_reference/text_stats.html)):\n",
    "- **[Flesch Reading Ease](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch.E2.80.93Kincaid_grade_level)**: Readability test used as a general-purpose standard in several languages, based on a weighted combination of avg. sentence length and avg. word length. Values usually fall in the range [0, 100], but may be arbitrarily negative in extreme cases. Higher value => easier text.\n",
    "- **[Gunning Fog Index](https://en.wikipedia.org/wiki/Gunning_fog_index)**: Readability test commonly used in Sweden on both English- and non-English-language texts, whose value estimates the difficulty of reading a foreign text. Higher value => more difficult text.\n",
    "- **[Smog Index](https://en.wikipedia.org/wiki/SMOG)**: Readability test commonly used in medical writing and the healthcare industry, whose value estimates the number of years of education required to understand a text similar to `flesch_kincaid_grade_level()` and intended as a substitute for `gunning_fog_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVGB2jYgH8-j",
    "outputId": "be7df341-9c96-47a1-f6d5-acc54928c4e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:textacy.text_stats.readability:SMOG index may be unreliable for n_sents < 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 6.345230909424329\n",
      "Flesch Grade Level: 8.509090909090908\n",
      "Smog Index: 10.504223727775692\n"
     ]
    }
   ],
   "source": [
    "from textacy.text_stats import TextStats\n",
    "from textacy import make_spacy_doc\n",
    "doc = make_spacy_doc(\"\"\"\n",
    "A month ago, new coronavirus cases in the United States were ticking steadily \n",
    "downward and the worst of a miserable summer surge fueled by the Delta variant \n",
    "appeared to be over. But as Americans travel this week to meet far-flung \n",
    "relatives for Thanksgiving dinner, new virus cases are rising once more, \n",
    "especially in the Upper Midwest and Northeast.\n",
    "\n",
    "Federal medical teams have been dispatched to Minnesota to help at overwhelmed \n",
    "hospitals. Michigan is enduring its worst case surge yet, with daily caseloads \n",
    "doubling since the start of November. Even New England, where vaccination rates \n",
    "are high, is struggling, with Vermont, Maine and New Hampshire trying to \n",
    "contain major outbreaks.\n",
    "\"\"\",  lang=\"en_core_web_sm\")\n",
    "ts = TextStats(doc)\n",
    "print(f\"Entropy: {ts.entropy}\")\n",
    "print(f\"Flesch Grade Level: {ts.flesch_kincaid_grade_level}\")\n",
    "print(f\"Smog Index: {ts.smog_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPUK2xmrJJ1q",
    "outputId": "65f6a0e5-a244-45ed-e8f7-465beeec0620"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:textacy.text_stats.readability:SMOG index may be unreliable for n_sents < 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 1.584962500721156\n",
      "Flesch Grade Level: -3.2049999999999983\n",
      "Smog Index: 3.1291\n"
     ]
    }
   ],
   "source": [
    "doc = make_spacy_doc(\"\"\"\n",
    "He do good.\n",
    "\"\"\",  lang=\"en_core_web_sm\")\n",
    "ts = TextStats(doc)\n",
    "print(f\"Entropy: {ts.entropy}\")\n",
    "print(f\"Flesch Grade Level: {ts.flesch_kincaid_grade_level}\")\n",
    "print(f\"Smog Index: {ts.smog_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxps-1VSKGD8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Using Textacy for Text Pre-Processing",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
